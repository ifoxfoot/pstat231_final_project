---
title: "Wildfire Exploration"
author: "Iris Foxfoot"
date: "2/3/2022"
output: 
  html_document:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  # if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
    toc_float: true #makes table of contents float while scrolling
    code_folding: hide #enables code folding
---

```{r setup, include=T}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)

library(tidyverse) #used for data wrangling and viz
library(here) #simplifies file paths
library(rsample) #used to split data
library(janitor) #used to clean data names
library(corrplot) #for correlation viz
library(tidymodels) #used for modeling workflow (MAYBE)
library(caret) #for cross validation
library(class) #for knn
library(gbm) #for boosted trees modeling
library(randomForest) #for random forest modeling
library(maptools)
library(RColorBrewer)
library(classInt)
library(ggplot2)
library(ggrepel)
library(mapproj)
library(viridis)

#turns scientific notation off
options(scipen = 100)

#some of our workflow includes randomness. here we set a seed so our workflow can be reproduced without worrying about random variation
set.seed(123) 
```

# Introduction

The purpose of this project is to use machine learning techniques to predict which wildfires will grow to a catastrophic size. Our most accurate model will then be used to assess how global climate change may influence a wildfires predicted size.

## Background

fire situation in North America, climate change, etc

## Why this model is useful

What will it be used for?

## Data and Packages Used

This dataset is available on Kaggle. It is a subset of larger fires 

Important attributes include

* `fire_size` - the size of the fire in acres

* `stat_cause_descr` - the cause of the fire

* `vegetation` - code corresponding to vegetation type

* `temp_cont` - temperature (Celsius) when the fire was contained

A complete codebook is available in the project file

```{r}
#read in dataset
us_wildfire <- read_csv(here("archive", "FW_Veg_Rem_Combined.csv"))

#have a peak at the data
glimpse(us_wildfire, 5)
```

# Methods

Overview of methods

## Cleaning Data
 
what we did to clean the data

```{r}
#First, there are a couple of junk columns in the data, so we select only the columns that mean something, then we use janitor's clean names function for lowercase snake col names
us_wildfire_clean <- us_wildfire %>% 
  dplyr::select(fire_name:remoteness) %>% 
  clean_names()

#we are interested in using weather to predict fire duration, so we filter out observations that do not have a weather file
us_wildfire_clean <- us_wildfire_clean %>% 
  filter(weather_file != "File Not Found")

#Here we label vegetation according to the provided codebook
us_wildfire_clean <- us_wildfire_clean %>% 
  mutate(vegetation_classed = case_when(
    vegetation == 12 ~ "Open Shrubland",
    vegetation == 15 ~ "Polar Desert/Rock/Ice",
    vegetation == 16 ~ "Secondary Tropical Evergreen Broadleaf Forest",
    vegetation == 4 ~ "Temperate Evergreen Needleleaf Forest TmpENF",
    vegetation == 9 ~ "C3 Grassland/Steppe",
    vegetation == 14 ~ "Desert"
  ))

#According the metadata for this data set, the vegetation was created by interpolating most likely vegetation based on latitude and longitude. The most common vegetation type is listed as "Polar Desert/Rock/Ice" and this seems very unlikely.

#There are some weather observations for which every weather field is 0. this seems unlikely. so we will replace them with NA
us_wildfire_clean <- us_wildfire_clean %>%
  mutate_at(vars(temp_pre_30:hum_cont), ~na_if(., 0.0000000))

#NOTE consider imputing missing data
  

#there are multiple date columns. however, since full dates are mostly missing, we will only keep month and year as variables
us_wildfire_clean <- us_wildfire_clean %>% 
  dplyr::select(-disc_clean_date, 
         -disc_date_pre, 
         -cont_clean_date, 
         -disc_pre_month,
         -disc_date_final,
         -cont_date_final,
         -putout_time)
```

## Exploratory Analysis

~ a few key graphs, exploring data

```{r}
#summarise acres per year burned
acres_per_year <- us_wildfire_clean %>% 
  group_by(disc_pre_year) %>% 
  summarise(acres_burned = sum(fire_size))

#fire size (finalized graph)
ggplot(data = acres_per_year) + 
  geom_point(aes(x = disc_pre_year, 
                 y = acres_burned, 
                 size = acres_burned, 
                 color = acres_burned)) +
  scale_color_continuous(high = "firebrick", low = "goldenrod1") +
  labs(x = "Year", y = "Total Acres Burned", 
       title = "Total acres burned per year from 1990 to 2015") +
  theme_minimal() +
  theme(legend.position = "none")

#remoteness (unfinalized)
ggplot(data = us_wildfire_clean) +
  geom_point(aes(x = remoteness, y = fire_size))

#most common causes of fire
fire_causes <- us_wildfire_clean %>% 
  group_by(stat_cause_descr) %>% 
  count()

#cause (finalized)
ggplot(data = fire_causes, aes(y = reorder(stat_cause_descr, n), x = n)) +
  geom_col(aes(fill = n)) +
  scale_fill_gradient(high = "firebrick", low = "goldenrod1") +
  labs(x = "Number of Fires", 
       y = "Cause",
       tite = "Number of fires per listed starting cause") +
  theme_minimal() +
  theme(legend.position = "none")

#distribution of fire size  
ggplot(data = us_wildfire_clean, aes(x = fire_size)) +  
  geom_histogram(bins = 100)

#It is likely that weather columns are correlated. to investigate this we create a correlation matrix

#create a dataframe of weather
weather <- us_wildfire_clean %>% 
  dplyr::select(temp_pre_30:prec_cont)

#create a correlation matrix (omitting NAs)
cor_matrix <- cor(weather, use = "complete.obs")

#create a visualization
corrplot(cor_matrix, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
#we can see that their is a strong correlation with each set of variables. i.e. temperature 7 days before the fire is correlated with temp 30 days before the fire. 

#consider PCA on wind, PCA on hum, PCA on temp
```

### Map of fires

#### The spatial distribution of wildifres 

```{r}
us_wildfire_clean$class_fac = factor(us_wildfire_clean$fire_size_class, levels = c("A", "B", "C", "D", "E", "F", "G"))

us <- map_data("world", 'usa')

state <- map_data("state")
ggplot() + 
  geom_polygon(data=state, aes(x=long, y=lat, group=group), color = "white", fill = "grey") + 
  geom_point(data = us_wildfire_clean, aes(x=longitude, y = latitude, color = class_fac)) +
  scale_color_brewer(palette = "YlOrRd")+
  ggtitle("US Wildfire Distribution")+
  guides(color=guide_legend(title="Wild Fire Scale"))+
  coord_map(projection = "sinusoidal", xlim=c(-120, -75), ylim = c(25, 50))
```

### When we divide it to three periods, we can see the wildfire risk has been growing in Western parts of the US.

```{r}
ggplot() + 
  geom_polygon(data=state, aes(x=long, y=lat, group=group), color = "white", fill = "grey") + 
  geom_point(data = us_wildfire_clean[which(us_wildfire_clean$wstation_byear < 1970),], aes(x=longitude, y = latitude, color = class_fac)) +
  scale_color_brewer(palette = "YlOrRd")+
  ggtitle("US Wildfire Distribution before 1970")+
  guides(color=guide_legend(title="Wild Fire Scale"))+
  coord_map(projection = "sinusoidal", xlim=c(-120, -75), ylim = c(25, 50))

ggplot() + 
  geom_polygon(data=state, aes(x=long, y=lat, group=group), color = "white", fill = "grey") + 
  geom_point(data = us_wildfire_clean[which(us_wildfire_clean$wstation_byear >= 1970 & us_wildfire_clean$wstation_byear < 2000),], aes(x=longitude, y = latitude, color = class_fac)) +
  scale_color_brewer(palette = "YlOrRd")+
  ggtitle("US Wildfire Distribution 1970-2000")+
  guides(color=guide_legend(title="Wild Fire Scale"))+
  coord_map(projection = "sinusoidal", xlim=c(-120, -75), ylim = c(25, 50))

ggplot() + 
  geom_polygon(data=state, aes(x=long, y=lat, group=group), color = "white", fill = "grey") + 
  geom_point(data = us_wildfire_clean[which(us_wildfire_clean$wstation_byear >= 200),], aes(x=longitude, y = latitude, color = class_fac)) +
  scale_color_brewer(palette = "YlOrRd")+
  ggtitle("US Wildfire Distribution after 2000")+
  guides(color=guide_legend(title="Wild Fire Scale"))+
  coord_map(projection = "sinusoidal", xlim=c(-120, -75), ylim = c(25, 50))
```

### Density graph?

```{r}
ggplot() + 
  geom_density(data= us_wildfire_clean[which(us_wildfire_clean$wstation_byear <= 1970 & us_wildfire_clean$fire_size > 100),], aes(x = fire_size, y=..density..),
               alpha=.3,
               colour="dodgerblue", fill="dodgerblue") + 
  geom_density(data= us_wildfire_clean[which(us_wildfire_clean$wstation_byear >= 1970 & us_wildfire_clean$fire_size > 100 & us_wildfire_clean$wstation_byear < 2000),], aes(x = fire_size, y=..density..),
               alpha=.3,
               colour="yellow3", fill="yellow3") + 
  geom_density(data= us_wildfire_clean[which(us_wildfire_clean$wstation_byear >= 2000 & us_wildfire_clean$fire_size > 100),], aes(x = fire_size, y=..density..),
               alpha=.3,
                 colour="firebrick3", fill="firebrick3") + 
  xlim(10000, 100000) + 
  ggtitle("Wildfire Severeity")
```
ideas for graphs
  - map of fires
  - vegetation
  - weather conditions

## Model Preparation

We split our data into 80% training and 20% testing data. Because fire size is heavily skewed towards smaller fires, we used stratified sampling.

There are xyz observations in the training set and xyz observations in the test set.

```{r}
#first we make a dataframe containing only variables we want to use to predict
fire_modeling_df <- us_wildfire_clean %>%
  dplyr::select(-fire_name, #remove fire name
         -fire_size_class, #remove fire size class
         -wstation_usaf, #remove weather station name
         -wstation_wban, #remove this (not described in codebook)
         -wstation_byear, #remove station year installed
         -wstation_eyear, #remove station year ended data recording
         -weather_file, #remove name of weather file
         -dstation_m, #remove distance of weather station to fire
         -vegetation #remove because we have it classed in another col
         ) %>% 
  mutate_if(is.character, as.factor) %>% #turn all character cols into factors
  mutate(disc_pre_year = as.factor(disc_pre_year)) %>% #turn year into factor
  na.omit(.) #taking out all NAs.
  
#define split parameters
us_wildfire_split <- fire_modeling_df %>% 
  initial_split(prop = 0.8, strata = "fire_size")

#write split data to data frames
fire_train <- training(us_wildfire_split)
fire_test <- testing(us_wildfire_split)

#set up folds in training data for cross validation
train_folds <- vfold_cv(fire_train, v = 10, repeats = 5)
```

## Modeling As a Regression Problem

```{r}
#if we want to use tidy models workflow

#set up a recipe for modeling NOT WORKING YET
regression_recipe <- recipe(
  #set up formuala to use
  fire_size ~ .,
  #specify use of training data
  data = fire_train,
  #One_hot encode all nominal predictors (dummy code them)
  step_dummy(stat_cause_descr,
             state,
             discovery_month,
             disc_pre_year,
             vegetation_classed, 
             one_hot = T) %>% 
  #scale numeric predictors
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())
  )
```

### K Nearest Neighbor

```{r}
#first we will use leave one out cross validation to determine the best number of neighbors to consider (k)

#knn works by calculating euclidean distance between observations, so all inputs have to be numeric. Therefore, we have to do some pre-model data changes to our training and test sets

#one-hot code training and test data
dummy <- dummyVars(" ~ .", data = fire_modeling_df)
knn_fire_train <- data.frame(predict(dummy, newdata = fire_train))
knn_fire_test <- data.frame(predict(dummy, newdata = fire_test))

#list possible number of nearest neighbors to be considered 
allK = 1:50

#create a vector of length 50 to save validation errors in future
k_validation_error = rep(NA, 50)

#YTrain is the true values for fire size on the training set 
YTrain = knn_fire_train$fire_size

#XTrain is the design matrix for training data
XTrain =  knn_fire_train %>% 
  select(-fire_size) %>% 
  scale(center = T, scale = T)


#YTest is the true value for fire_size on the test set
YTest = knn_fire_test$fire_size

#Xtest is the design matrix for test data
XTest = knn_fire_test %>% 
  select(-fire_size) %>% 
  scale(center = TRUE, scale = TRUE)

# For each number in allK, use LOOCV to find a validation error  
for (i in allK){  
  # Loop through different number of neighbors
  # Predict on the left-out validation set
  pred.Yval = knn.cv(train = XTrain, cl =YTrain, k = i) 
  # Combine all validation errors
  k_validation_error[i] = mean(pred.Yval != YTrain)
}


```

### Boosted Tree

```{r}
#train boosted tree model
fire_size_boost = gbm(fire_size~., 
                      data = fire_train,
                      n.trees = 500, 
                      interaction.depth = 4
                    )

#the model summary creates a pretty visualization
summary(fire_size_boost)

#calculate training error

#predict values using training data
predictions_train_boost <- predict(fire_size_boost, data = fire_train)

#calculate rmse for training data
RMSE(predictions_train_boost, fire_train$fire_size)

#calculate test error

#predict values using test data
predictions_test_boost <- predict(fire_size_boost, data = fire_test)

#calculate rmse for training data
RMSE(predictions_test_boost, fire_test$fire_size)
```

### Random Forest

```{r}
#train model
fire_size_rf = randomForest(fire_size ~ ., #writing the formula
                          data = fire_train, #specifying training data to be used
                          mtry = 9, #setting number of variables to randomly sample per each split
                          ntree= 500, #setting number of trees
                          mode = "regression", #specifying regression
                          na.action = na.omit, #specifying what to do with NAs
                          importance = TRUE #specifying importance of variables should be assessed
                          )
#plot error
plot(fire_size_rf)

#plot variable importance
varImpPlot(fire_size_rf, 
           sort = T, 
           main = "Variable Importance for fire size random forest model", 
           n.var = 5)

#calculate training error

#predict values using training data
predictions_train_rf <- predict(fire_size_rf, data = fire_train)

#calculate rmse for training data
RMSE(predictions_train_rf, fire_train$fire_size)

#calculate test error

#predict values using test data
predictions_test_rf <- predict(fire_size_rf, data = fire_test)

#calculate rmse for training data
RMSE(predictions_test_rf, fire_test$fire_size)
```

what type of modeling will we do? we must try 4 types

- K nearest Neighbor
- neural network

# Results

predictions without climate change

How do predictions change with 2.5C increase in temp?

# Conclusion


