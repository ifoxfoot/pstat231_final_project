---
title: "Wildfire Exploration"
author: "Iris Foxfoot"
date: "2/3/2022"
output: 
  html_document:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
    toc_float: true #makes table of contents float while scrolling
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse) #used for data wrangling and viz
library(here) #simplifies file paths
library(rsample) #used to split data
library(janitor) #used to clean data names
library(tidymodels)
#library(skimr)
```

# Introduction

The purpose of this project is to use machine learning techniques to predict which wildfires will grow to a catastrophic size. Our most accurate model will then be used to assess how global climate change may influence a wildfires predicted size.

## Background

fire situation in North America, climate change, etc

## Why this model is useful

What will it be used for?

## Data and Packages Used

This dataset is available on Kaggle. It is a subset of larger fires 

Important attributes include

*`fire_size` - the size of the fire in acres

*`stat_cause_descr` - the cause of the fire

*`vegetation` - code corresponding to vegetation type

*`temp_cont` - temperature (Celsius) when the fire was contained

A complete codebook is available in the project file

```{r}
#read in dataset
us_wildfire <- read_csv(here("archive", "FW_Veg_Rem_Combined.csv"))

#have a peak at the data
glimpse(us_wildfire, 5)
```

# Methods

Overview of methods

## Cleaning Data
 
what we did to clean the data

```{r}
#First, there are a couple of junk columns in the data, so we select only the columns that mean something
us_wildfire_clean <- us_wildfire %>% 
  select(fire_name:remoteness) %>% 
  clean_names()

#we are interested in using weather to predict fire duration, so we select observations that have a weather file
us_wildfire_clean <- us_wildfire_clean %>% 
  filter(weather_file != "File Not Found")

#NOTE. we should correct vegetation with accurate label
us_wildfire_clean$vegetation <- as.factor(us_wildfire_clean$vegetation)

#NOTE. There are some remaining observations for which every weather field is 0. this seems unlikely

#NOTE. The put_out time is incorrectly calculated so we must calculate it

#there are multiple date columns. we only want to keep the ones with "final" in the name
us_wildfire_clean <- us_wildfire_clean %>% 
  select(-disc_clean_date, 
         -disc_date_pre, 
         -cont_clean_date, 
         -disc_pre_month)
```

## Exploratory Analysis

~ a few key graphs, exploring data

```{r}
#turns scientific notation off
options(scipen = 100)

#summarise acres per year burned
acres_per_year <- us_wildfire_clean %>% 
  group_by(disc_pre_year) %>% 
  summarise(acres_burned = sum(fire_size))

#fire size (finalized graph)
ggplot(data = acres_per_year) + 
  geom_point(aes(x = disc_pre_year, 
                 y = acres_burned, 
                 size = acres_burned, 
                 color = acres_burned)) +
  scale_color_continuous(high = "firebrick", low = "goldenrod1") +
  labs(x = "Year", y = "Total Acres Burned", 
       title = "Total acres burned per year from 1990 to 2015") +
  theme_minimal() +
  theme(legend.position = "none")

#remoteness (unfinalized)
ggplot(data = us_wildfire_clean) +
  geom_point(aes(x = remoteness, y = fire_size))

#most common causes of fire
fire_causes <- us_wildfire_clean %>% 
  group_by(stat_cause_descr) %>% 
  count()

#cause (finalized)
ggplot(data = fire_causes, aes(y = reorder(stat_cause_descr, n), x = n)) +
  geom_col(aes(fill = n)) +
  scale_fill_gradient(high = "firebrick", low = "goldenrod1") +
  labs(x = "Number of Fires", 
       y = "Cause",
       tite = "Number of fires per listed starting cause") +
  theme_minimal() +
  theme(legend.position = "none")

#distribution of fire size  
ggplot(data = us_wildfire_clean, aes(x = fire_size)) +  
  geom_histogram(bins = 100)
```

ideas for graphs
  - map of fires
  - duration of fires
  - vegetation
  - weather conditions

## Splitting Data

We split our data into 80% training and 20% testing data. Because fire size is heavily skewed towards smaller fires, we used stratified sampling.

There are 32903 observations in the training set and 8229 observations in the test dataset.

```{r}
#define split parameters
us_wildfire_split <- us_wildfire_clean %>% 
  initial_split(prop = 0.8, strata = "fire_size")

#write split data to data frames
fire_train <- training(us_wildfire_split)
fire_test <- testing(us_wildfire_split)
```

## Modeling

### Model Preparation

To begin modeling we first select independent variables we wish to use, and we also set up cross validation infrastucture. For cross validation we will use repeated K folding.

```{r}
#select independent variables to use in modeling
independent_vars <- us_wildfire_clean %>%
  select(-fire_name, #remove fire name
         -fire_size, #remove fire size (we are predicting this!)
         -fire_size_class, #remove fire size class
         -wstation_usaf, #remove weather station name
         -wstation_wban, #remove this (not described in codebook)
         -wstation_byear, #remove station year installed
         -wstation_eyear, #remove station year ended data recording
         -weather_file, #remove name of weather file
         -dstation_m, #remove distance of weather station to fire
         -cont_date_final, #NOTE, not sure if we should remove this and below
         -disc_date_final,
         -temp_cont,
         -wind_cont,
         -hum_cont,
         -prec_cont,
         -putout_time,
         ) %>%  
  names(.) 

#show final list of predictors
print(independent_vars)

#set up a recipe for modeling NOT WORKING YET
recipe <- recipe(fire_size ~ stat_cause_descr + latitude + longitude + state + discovery_month + disc_pre_year + vegetation + fire_mag + temp_pre_30 + temp_pre_15 + temp_pre_7 + wind_pre_30 + wind_pre_15 + wind_pre_7 + hum_pre_30 + hum_pre_15 + hum_pre_7 + prec_pre_30 + prec_pre_15 + prec_pre_7 + remoteness, data = fire_train)
```

```{r}
#set up folds in training data for cross validation
train_folds <- vfold_cv(fire_train, v = 10, repeats = 5)
```

### Linear Regression

```{r}
#run a lm
fire_size_lm <- lm(fire_size ~ stat_cause_descr + latitude + longitude + state + discovery_month + disc_pre_year + vegetation + fire_mag + temp_pre_30 + temp_pre_15 + temp_pre_7 + wind_pre_30 + wind_pre_15 + wind_pre_7 + hum_pre_30 + hum_pre_15 + hum_pre_7 + prec_pre_30 + prec_pre_15 + prec_pre_7 + remoteness, data = fire_train)

#summarise the model
summary(fire_size_lm)

#calculate mse
mean(fire_size_lm$residuals^2)
```


what type of modeling will we do? we must try 4 types

- multiple linear regression
- K nearest Neighbor
- Random Forest
- neural network


## Cross Validation

Which model is best?

## Final Model Selection

## Test Error

# Results

predictions without climate change

How do predictions change with 2.5C increase in temp?

# Conclusion


