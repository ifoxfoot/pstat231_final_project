---
title: "Wildfire Exploration"
author: "Iris Foxfoot"
date: "2/3/2022"
output: 
  html_document:
    toc: true # table of content true
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    theme: united  # many options for theme, this one is my favorite.
    highlight: tango  # specifies the syntax highlighting style
    toc_float: true #makes table of contents float while scrolling
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse) #used for data wrangling and viz
library(here) #simplifies file paths
library(rsample) #used to split data
```

# Introduction

The purpose of this project is to use machine learning techniques to predict which wildfires will grow to a catastrophic size. Our most accurate model will then be used to assess how global climate change may influence a wildfires predicted size.

## Background

fire situation in North America, climate change, etc

## Why this model is useful

What will it be used for?

## Data and Packages Used

This dataset is available on Kaggle. It is a subset of larger fires 

Important attributes include

*`fire_size` - the size of the fire in acres

*`stat_cause_descr` - the cause of the fire

*`Vegetation` - code corresponding to vegetation type

*`Temp_cont` - temperature (Celsius) when the fire was contained

A complete codebook is available in the project file

```{r}
#read in dataset
us_wildfire <- read_csv(here("archive", "FW_Veg_Rem_Combined.csv"))

#show head
slice_max(us_wildfire, 5)
```

# Methods

Overview of methods

## Cleaning Data
 
what we did to clean the data

```{r}
#First, there are a couple of junk columns in the data, so we select only the columns that mean something
us_wildfire_clean <- us_wildfire %>% 
  select(fire_name:remoteness)

#we are interested in using weather to predict fire duration, so we select observations that have a weather file
us_wildfire_clean <- us_wildfire_clean %>% 
  filter(weather_file != "File Not Found")

#NOTE. There are some remaining observations for which every weather field is 0. this seems unlikely

#NOTE. The put_out time is incorrectly calculated so we must calculate it

```

## Exploratory Analysis

~ a few key graphs, exploring data

```{r}
#turns scientific notation off
options(scipen = 100)

#summarise acres per year burned
acres_per_year <- us_wildfire_clean %>% 
  group_by(disc_pre_year) %>% 
  summarise(acres_burned = sum(fire_size))

#fire size (finalized graph)
ggplot(data = acres_per_year) + 
  geom_point(aes(x = disc_pre_year, 
                 y = acres_burned, 
                 size = acres_burned, 
                 color = acres_burned)) +
  scale_color_continuous(high = "firebrick", low = "goldenrod1") +
  labs(x = "Year", y = "Total Acres Burned", 
       title = "Total acres burned per year from 1990 to 2015") +
  theme_minimal() +
  theme(legend.position = "none")

#remoteness (unfinalized)
ggplot(data = us_wildfire_clean) +
  geom_point(aes(x = remoteness, y = fire_size))

#most common causes of fire
fire_causes <- us_wildfire_clean %>% 
  group_by(stat_cause_descr) %>% 
  count()

#cause (finalized)
ggplot(data = fire_causes, aes(y = reorder(stat_cause_descr, n), x = n)) +
  geom_col(aes(fill = n)) +
  scale_fill_gradient(high = "firebrick", low = "goldenrod1") +
  labs(x = "Number of Fires", 
       y = "Cause",
       tite = "Number of fires per listed starting cause") +
  theme_minimal() +
  theme(legend.position = "none")

#distribution of fire size  
ggplot(data = us_wildfire_clean, aes(x = fire_size)) +  
  geom_histogram(bins = 100)

```

ideas for graphs
  - map of fires
  - duration of fires
  - vegetation
  - weather conditions

## Splitting Data

We split our data into 80% training and 20% testing data. Because fire size is heavily skewed towards smaller fires, we used stratified sampling.

There are 32903 observations in the training set and 8229 observations in the test dataset.

```{r}
#define split parameters
us_wildfire_split <- us_wildfire_clean %>% 
  initial_split(prop = 0.8, strata = "fire_size")

#write split data to data frames
fire_train <- training(us_wildfire_split)
fire_test <- testing(us_wildfire_split)
```

## Modeling

what type of modeling will we do? we must try 4 types

- multiple linear regression
- K nearest Neighbor
- Random Forest
- neural network


## Cross Validation

Which model is best?

## Final Model Selection

## Test Error

# Results

predictions without climate change

How do predictions change with 2.5C increase in temp?

# Conclusion


